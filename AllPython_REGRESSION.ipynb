{"metadata":{"anaconda-cloud":{},"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Notes\nPandas attributes and methods: </br>\n</br>\npd.read.csv() - read csv files</br>\ndf.head() - take a look of the dataframe</br>\ndf.columns - retrieve colum names of a dataframe</br>\ndf.columns.str.lower() - lowercase all the letters</br>\ndf.columns.str.replace(' ', '_') - replace the space separator</br>\ndf.dtypes - retrieve data types of all features</br>\ndf.index - retrive indices of a dataframe</br>","metadata":{"tags":[]}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n%matplotlib inline","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('data.csv')\nlen(df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns = df.columns.str.lower().str.replace(' ', '_')\n\nstring_columns = list(df.dtypes[df.dtypes == 'object'].index)\n\nfor col in string_columns:\n    df[col] = df[col].str.lower().str.replace(' ', '_')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory data analysis","metadata":{}},{"cell_type":"markdown","source":"### Notes</br>\nPandas attributes and methods:</br>\n</br>\ndf[col].unique() - returns a list of unique values in the series</br>\ndf[col].nunique() - returns the number of unique values in the series</br>\ndf.isnull().sum() - retunrs the number of null values in the dataframe</br>\nMatplotlib and seaborn methods:</br>\n</br>\n%matplotlib inline - assure that plots are displayed in jupyter notebooks cells</br>\nsns.histplot() - show the histogram of a series</br>\nNumpy methods:</br>\n</br>\nnp.log1p() - applies log transformation to a variable and adds one to each result</br>\nLong-tail distributions usually confuse the ML models, so the recommendation is to transform the target variable distribution to a normal one whenever possible.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(6, 4))\n\nsns.histplot(df.msrp, bins=40, color='black', alpha=1)\nplt.ylabel('Frequency')\nplt.xlabel('Price')\nplt.title('Distribution of prices')\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(6, 4))\n\nsns.histplot(df.msrp[df.msrp < 100000], bins=40, color='black', alpha=1)\nplt.ylabel('Frequency')\nplt.xlabel('Price')\nplt.title('Distribution of prices')\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_price = np.log1p(df.msrp)\n\nplt.figure(figsize=(6, 4))\n\nsns.histplot(log_price, bins=40, color='black', alpha=1)\nplt.ylabel('Frequency')\nplt.xlabel('Log(Price + 1)')\nplt.title('Distribution of prices after log tranformation')\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Validation framework","metadata":{}},{"cell_type":"markdown","source":"### Notes\nIn general, the dataset is split into three parts: training, validation, and test. For each partition, we need to obtain feature matrices (X) and y vectors of targets. First, the size of partitions is calculated, records are shuffled to guarantee that values of the three partitions contain non-sequential records of the dataset, and the partitions are created with the shuffled indices.</br>\n</br>\nPandas attributes and methods:</br>\n</br>\ndf.iloc[] - returns subsets of records of a dataframe, being selected by numerical indices</br>\ndf.rest_index() - restate the orginal indices</br>\ndel df[col] - eliminates target variable</br>\nNumpy methods:</br>\n</br>\nnp.arrange() - retruns an array of numbers</br>\nnp.random.shuffle() - retturns a suffled array</br>\nnp.random.seed() - set a seed</br>","metadata":{}},{"cell_type":"code","source":"np.random.seed(2)\n\nn = len(df)\n\nn_val = int(0.2 * n)\nn_test = int(0.2 * n)\nn_train = n - (n_val + n_test)\n\nidx = np.arange(n)\nnp.random.shuffle(idx)\n\ndf_shuffled = df.iloc[idx]\n\ndf_train = df_shuffled.iloc[:n_train].copy()\ndf_val = df_shuffled.iloc[n_train:n_train+n_val].copy()\ndf_test = df_shuffled.iloc[n_train+n_val:].copy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_orig = df_train.msrp.values\ny_val_orig = df_val.msrp.values\ny_test_orig = df_test.msrp.values\n\ny_train = np.log1p(df_train.msrp.values)\ny_val = np.log1p(df_val.msrp.values)\ny_test = np.log1p(df_test.msrp.values)\n\ndel df_train['msrp']\ndel df_val['msrp']\ndel df_test['msrp']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Linear Regression","metadata":{}},{"cell_type":"markdown","source":"### Notes\nModel for solving regression tasks, in which the objective is to adjust a line for the data and make predictions on new values. The input of this model is the feature matrix and a y vector of predictions is obtained, trying to be as close as possible to the actual y values. The LR formula is the sum of the bias term (WO), which refers to the predictions if there is no information, and each of the feature values times their corresponding weights. We need to assure that the result is showed on the untransformed scale. </br>","metadata":{}},{"cell_type":"code","source":"def train_linear_regression(X, y):\n    ones = np.ones(X.shape[0])\n    X = np.column_stack([ones, X])\n\n    XTX = X.T.dot(X)\n    XTX_inv = np.linalg.inv(XTX)\n    w = XTX_inv.dot(X.T).dot(y)\n    \n    return w[0], w[1:]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Baseline solution","metadata":{}},{"cell_type":"markdown","source":"### Notes\nThe LR model obtained in the previous section was used with the dataset of car price prediction. For this model, only the numerical variables were considered. The training data was pre-processed, replacing the NaN values with 0, in such a way that these values were omitted by the model. Then, the model was trained and it allowed to make predictions on new data. Finally, distributions of y target variable and predictions were compared by plotting their histograms.","metadata":{}},{"cell_type":"code","source":"base = ['engine_hp', 'engine_cylinders', 'highway_mpg', 'city_mpg', 'popularity']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_X(df):\n    df_num = df[base]\n    df_num = df_num.fillna(0)\n    X = df_num.values\n    return X","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = prepare_X(df_train)\nw_0, w = train_linear_regression(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = w_0 + X_train.dot(w)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(6, 4))\n\nsns.histplot(y_train, label='target', color='#222222', alpha=0.6, bins=40)\nsns.histplot(y_pred, label='prediction', color='#aaaaaa', alpha=0.8, bins=40)\n\nplt.legend()\n\nplt.ylabel('Frequency')\nplt.xlabel('Log(Price + 1)')\nplt.title('Predictions vs actual distribution')\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmse(y, y_pred):\n    error = y_pred - y\n    mse = (error ** 2).mean()\n    return np.sqrt(mse)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rmse(y_train, y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_val = prepare_X(df_val)\ny_pred = w_0 + X_val.dot(w)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rmse(y_val, y_pred)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Simple feature engineering","metadata":{}},{"cell_type":"markdown","source":"### Notes\nThe feature age of the car was included in the dataset, obtained with the subtraction of the maximum year of cars and each of the years of cars. This new feature improved the model performance, measured with the RMSE and comparing the distributions of y target variable and predictions.","metadata":{}},{"cell_type":"code","source":"def prepare_X(df):\n    df = df.copy()\n    features = base.copy()\n\n    df['age'] = 2017 - df.year\n    features.append('age')\n\n    df_num = df[features]\n    df_num = df_num.fillna(0)\n    X = df_num.values\n    return X","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = prepare_X(df_train)\nw_0, w = train_linear_regression(X_train, y_train)\n\ny_pred = w_0 + X_train.dot(w)\nprint('train', rmse(y_train, y_pred))\n\nX_val = prepare_X(df_val)\ny_pred = w_0 + X_val.dot(w)\nprint('validation', rmse(y_val, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(6, 4))\n\n\nsns.histplot(y_val, label='target', color='#222222', alpha=0.6, bins=40)\nsns.histplot(y_pred, label='prediction', color='#aaaaaa', alpha=0.8, bins=40)\n\nplt.legend()\n\nplt.ylabel('Frequency')\nplt.xlabel('Log(Price + 1)')\nplt.title('Predictions vs actual distribution')\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['make'].value_counts().head(5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_X(df):\n    df = df.copy()\n    features = base.copy()\n\n    df['age'] = 2017 - df.year\n    features.append('age')\n\n    for v in [2, 3, 4]:\n        feature = 'num_doors_%s' % v\n        df[feature] = (df['number_of_doors'] == v).astype(int)\n        features.append(feature)\n\n    for v in ['chevrolet', 'ford', 'volkswagen', 'toyota', 'dodge']:\n        feature = 'is_make_%s' % v\n        df[feature] = (df['make'] == v).astype(int)\n        features.append(feature)\n\n    df_num = df[features]\n    df_num = df_num.fillna(0)\n    X = df_num.values\n    return X","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = prepare_X(df_train)\nw_0, w = train_linear_regression(X_train, y_train)\n\ny_pred = w_0 + X_train.dot(w)\nprint('train:', rmse(y_train, y_pred))\n\nX_val = prepare_X(df_val)\ny_pred = w_0 + X_val.dot(w)\nprint('validation:', rmse(y_val, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['engine_fuel_type'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_X(df):\n    df = df.copy()\n    features = base.copy()\n\n    df['age'] = 2017 - df.year\n    features.append('age')\n    \n    for v in [2, 3, 4]:\n        feature = 'num_doors_%s' % v\n        df[feature] = (df['number_of_doors'] == v).astype(int)\n        features.append(feature)\n\n    for v in ['chevrolet', 'ford', 'volkswagen', 'toyota', 'dodge']:\n        feature = 'is_make_%s' % v\n        df[feature] = (df['make'] == v).astype(int)\n        features.append(feature)\n\n    for v in ['regular_unleaded', 'premium_unleaded_(required)', \n              'premium_unleaded_(recommended)', 'flex-fuel_(unleaded/e85)']:\n        feature = 'is_type_%s' % v\n        df[feature] = (df['engine_fuel_type'] == v).astype(int)\n        features.append(feature)\n        \n    df_num = df[features]\n    df_num = df_num.fillna(0)\n    X = df_num.values\n    return X","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = prepare_X(df_train)\nw_0, w = train_linear_regression(X_train, y_train)\n\ny_pred = w_0 + X_train.dot(w)\nprint('train:', rmse(y_train, y_pred))\n\nX_val = prepare_X(df_val)\ny_pred = w_0 + X_val.dot(w)\nprint('validation:', rmse(y_val, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['transmission_type'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_X(df):\n    df = df.copy()\n    features = base.copy()\n\n    df['age'] = 2017 - df.year\n    features.append('age')\n    \n    for v in [2, 3, 4]:\n        feature = 'num_doors_%s' % v\n        df[feature] = (df['number_of_doors'] == v).astype(int)\n        features.append(feature)\n\n    for v in ['chevrolet', 'ford', 'volkswagen', 'toyota', 'dodge']:\n        feature = 'is_make_%s' % v\n        df[feature] = (df['make'] == v).astype(int)\n        features.append(feature)\n\n    for v in ['regular_unleaded', 'premium_unleaded_(required)', \n              'premium_unleaded_(recommended)', 'flex-fuel_(unleaded/e85)']:\n        feature = 'is_type_%s' % v\n        df[feature] = (df['engine_fuel_type'] == v).astype(int)\n        features.append(feature)\n\n    for v in ['automatic', 'manual', 'automated_manual']:\n        feature = 'is_transmission_%s' % v\n        df[feature] = (df['transmission_type'] == v).astype(int)\n        features.append(feature)\n        \n    df_num = df[features]\n    df_num = df_num.fillna(0)\n    X = df_num.values\n    return X","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = prepare_X(df_train)\nw_0, w = train_linear_regression(X_train, y_train)\n\ny_pred = w_0 + X_train.dot(w)\nprint('train:', rmse(y_train, y_pred))\n\nX_val = prepare_X(df_val)\ny_pred = w_0 + X_val.dot(w)\nprint('validation:', rmse(y_val, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['driven_wheels'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['market_category'].value_counts().head(5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['vehicle_size'].value_counts().head(5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['vehicle_style'].value_counts().head(5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_X(df):\n    df = df.copy()\n    features = base.copy()\n\n    df['age'] = 2017 - df.year\n    features.append('age')\n    \n    for v in [2, 3, 4]:\n        feature = 'num_doors_%s' % v\n        df[feature] = (df['number_of_doors'] == v).astype(int)\n        features.append(feature)\n\n    for v in ['chevrolet', 'ford', 'volkswagen', 'toyota', 'dodge']:\n        feature = 'is_make_%s' % v\n        df[feature] = (df['make'] == v).astype(int)\n        features.append(feature)\n\n    for v in ['regular_unleaded', 'premium_unleaded_(required)', \n              'premium_unleaded_(recommended)', 'flex-fuel_(unleaded/e85)']:\n        feature = 'is_type_%s' % v\n        df[feature] = (df['engine_fuel_type'] == v).astype(int)\n        features.append(feature)\n\n    for v in ['automatic', 'manual', 'automated_manual']:\n        feature = 'is_transmission_%s' % v\n        df[feature] = (df['transmission_type'] == v).astype(int)\n        features.append(feature)\n\n    for v in ['front_wheel_drive', 'rear_wheel_drive', 'all_wheel_drive', 'four_wheel_drive']:\n        feature = 'is_driven_wheens_%s' % v\n        df[feature] = (df['driven_wheels'] == v).astype(int)\n        features.append(feature)\n\n    for v in ['crossover', 'flex_fuel', 'luxury', 'luxury,performance', 'hatchback']:\n        feature = 'is_mc_%s' % v\n        df[feature] = (df['market_category'] == v).astype(int)\n        features.append(feature)\n\n    for v in ['compact', 'midsize', 'large']:\n        feature = 'is_size_%s' % v\n        df[feature] = (df['vehicle_size'] == v).astype(int)\n        features.append(feature)\n\n    for v in ['sedan', '4dr_suv', 'coupe', 'convertible', '4dr_hatchback']:\n        feature = 'is_style_%s' % v\n        df[feature] = (df['vehicle_style'] == v).astype(int)\n        features.append(feature)\n\n    df_num = df[features]\n    df_num = df_num.fillna(0)\n    X = df_num.values\n    return X","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = prepare_X(df_train)\nw_0, w = train_linear_regression(X_train, y_train)\n\ny_pred = w_0 + X_train.dot(w)\nprint('train:', rmse(y_train, y_pred))\n\nX_val = prepare_X(df_val)\ny_pred = w_0 + X_val.dot(w)\nprint('validation:', rmse(y_val, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w_0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Regularization","metadata":{}},{"cell_type":"markdown","source":"### Notes\nIf the feature matrix has duplicated columns, it does not have an inverse matrix. But, sometimes this error could be passed if certain values are slightly different between duplicated columns.\n\nSo, if we apply the normal equation with this feature matrix, the values associated with duplicated columns are very large, which decreases the model performance. To solve this issue, one alternative is adding a small number to the diagonal of the feature matrix, which corresponds to regularization.\n\nThis technique works because the addition of small values to the diagonal makes it less likely to have duplicated columns. The regularization value is a parameter of the model. After applying regularization the model performance improved.","metadata":{}},{"cell_type":"code","source":"def train_linear_regression_reg(X, y, r=0.0):\n    ones = np.ones(X.shape[0])\n    X = np.column_stack([ones, X])\n\n    XTX = X.T.dot(X)\n    reg = r * np.eye(XTX.shape[0])\n    XTX = XTX + reg\n\n    XTX_inv = np.linalg.inv(XTX)\n    w = XTX_inv.dot(X.T).dot(y)\n    \n    return w[0], w[1:]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = prepare_X(df_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for r in [0, 0.001, 0.01, 0.1, 1, 10]:\n    w_0, w = train_linear_regression_reg(X_train, y_train, r=r)\n    print('%5s, %.2f, %.2f, %.2f' % (r, w_0, w[13], w[21]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = prepare_X(df_train)\nw_0, w = train_linear_regression_reg(X_train, y_train, r=0)\n\ny_pred = w_0 + X_train.dot(w)\nprint('train', rmse(y_train, y_pred))\n\nX_val = prepare_X(df_val)\ny_pred = w_0 + X_val.dot(w)\nprint('val', rmse(y_val, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = prepare_X(df_train)\nw_0, w = train_linear_regression_reg(X_train, y_train, r=0.01)\n\ny_pred = w_0 + X_train.dot(w)\nprint('train', rmse(y_train, y_pred))\n\nX_val = prepare_X(df_val)\ny_pred = w_0 + X_val.dot(w)\nprint('val', rmse(y_val, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = prepare_X(df_train)\nX_val = prepare_X(df_val)\n\nfor r in [0.000001, 0.0001, 0.001, 0.01, 0.1, 1, 5, 10]:\n    w_0, w = train_linear_regression_reg(X_train, y_train, r=r)\n    y_pred = w_0 + X_val.dot(w)\n    print('%6s' %r, rmse(y_val, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = prepare_X(df_train)\nw_0, w = train_linear_regression_reg(X_train, y_train, r=0.01)\n\nX_val = prepare_X(df_val)\ny_pred = w_0 + X_val.dot(w)\nprint('validation:', rmse(y_val, y_pred))\n\nX_test = prepare_X(df_test)\ny_pred = w_0 + X_test.dot(w)\nprint('test:', rmse(y_test, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using the model","metadata":{}},{"cell_type":"code","source":"i = 2\nad = df_test.iloc[i].to_dict()\nad","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = prepare_X(pd.DataFrame([ad]))[0]\ny_pred = w_0 + X_test.dot(w)\nsuggestion = np.expm1(y_pred)\nsuggestion","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Questions\n\n* In this project, we We included only 5 top features. What happens if we include 10?\n\n\n### Other projects\n\nHere are other datasets that you can play with to learn more about the topic:\n\n* [California housing dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html) - predict the price of a house\n* [Student Performance Data Set](https://archive.ics.uci.edu/ml/datasets/Student+Performance) - predict the performance of students\n* UCI ML Repository contains a lot of other datasets suitable for practicing regression - https://archive.ics.uci.edu/ml/datasets.php?task=reg\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}